{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpBVeU0XX8Uk"
   },
   "source": [
    "<h1>12장 생성 모델 미세 튜닝하기</h1>\n",
    "<i>생성 LLM을 미세 튜닝하기 위한 두 단계 접근 방식에 대한 탐험</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter12.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "이 노트북은 <[핸즈온 LLM](https://tensorflow.blog/handson-llm/)> 책 12장의 코드를 담고 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
    "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGw_POflgIII"
   },
   "source": [
    "### [선택사항] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>에서 패키지 선택하기\n",
    "\n",
    "\n",
    "이 노트북을 구글 코랩에서 실행한다면 다음 코드 셀을 실행하여 이 노트북에서 필요한 패키지를  설치하세요.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UatvUaylSbPZ"
   },
   "source": [
    "*trl 버전 0.17에서 SFTTrainer를 실행할 때 KeyError: 'completion'가 발생하므로 trl 버전을 0.16.1로 고정한다.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H9EuD4pvgIII"
   },
   "source": [
    "%%capture\n",
    "!pip install datasets bitsandbytes trl==0.16.1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5luSSUAu_6d"
   },
   "source": [
    "## 지도 학습 미세 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPtcbw38_hVi"
   },
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SqeZchJiOXdd"
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# 채팅 템플릿을 사용하기 위해 토크나이저를 로드합니다.\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"TinyLlama의 <|user|> 템플릿으로 프롬프트를 포맷팅합니다\"\"\"\n",
    "\n",
    "    # 채팅 템플릿 구성\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# 데이터를 로드하고 TinyLlama 템플릿을 적용합니다.\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n",
    "      .shuffle(seed=42)\n",
    "      .select(range(3_000))\n",
    ")\n",
    "dataset = dataset.map(format_prompt).remove_columns(['messages'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtl2xZptgyDf",
    "outputId": "13441777-3923-4755-c71e-e2bff795e4df"
   },
   "source": [
    "# 프롬프트 예시\n",
    "print(dataset[\"text\"][2576])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyuLZGizDqUB"
   },
   "source": [
    "### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M95Y207T7wSp"
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-비트 양자화 설정 - QLoRA의 Q 단계\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-비트 정밀도 모델 로드\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 양자화 종류\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # 계산 dtype\n",
    "    bnb_4bit_use_double_quant=True,  # 이중 양자화 적용\n",
    ")\n",
    "\n",
    "# 모델을 로드하고 GPU에서 훈련합니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # 일반적인 SFT에서는 다음을 삭제하세요.\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# LLaMA 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1iGIch-sAMC"
   },
   "source": [
    "### 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86o1T5n4DziD"
   },
   "source": [
    "#### LoRA 설정"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0tYs1ZhYDyw9"
   },
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,  # LoRA 스케일링\n",
    "    lora_dropout=0.1,  # LoRA 층의 드롭아웃\n",
    "    r=64,  # 랭크\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # 대상 층\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# 훈련을 위한 모델 준비\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhbh7kKuD24o"
   },
   "source": [
    "#### 훈련 설정"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TwxZkx80G6bO"
   },
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# 훈련 매개변수\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtwIo5a0D6f1"
   },
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B2D7RVihsE7Z",
    "outputId": "78c18382-1a6d-4e83-8ca9-3dbc66fa7876"
   },
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# 지도 미세 튜닝 매개변수 지정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "\n",
    "    # 일반적인 SFT에서는 다음을 삭제하세요.\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "trainer.train()\n",
    "\n",
    "# QLoRA 가중치 저장\n",
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsIBfv1PsId-"
   },
   "source": [
    "### 어댑터 병합"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M6cPdde4Z-ks"
   },
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA와 베이스 모델을 병합합니다.\n",
    "merged_model = model.merge_and_unload()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPRYGimIsM2-"
   },
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15dJC3ZrdVnK",
    "outputId": "74525191-4468-49ef-ea0c-3f23e431526d"
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 사전에 정의된 프롬프트 템플릿을 사용합니다.\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# 인스트럭션 튜닝된 모델을 실행합니다.\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JNfYZe9vCb8"
   },
   "source": [
    "## 선호도 튜닝 (PPO/DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar2h9kZ9qmEG"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlbPVO_aac33",
    "outputId": "e238d3de-2421-4464-8687-76fb4b5640fa"
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"TinyLlama의 <|user|> 템플릿을 사용해 프롬프트를 구성합니다\"\"\"\n",
    "\n",
    "    # 템플릿 포맷팅\n",
    "    system = \"<|system|>\\n\" + example['system'] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example['input'] + \"</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example['chosen'] + \"</s>\\n\"\n",
    "    rejected = example['rejected'] + \"</s>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# 데이터셋에 템플릿을 적용하고 비교적 짧은 대답을 선택합니다\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r:\n",
    "        r[\"status\"] != \"tie\" and\n",
    "        r[\"chosen_score\"] >= 8 and\n",
    "        not r[\"in_gsm8k_train\"]\n",
    ")\n",
    "dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)\n",
    "dpo_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkCJ4CO5sQG6"
   },
   "source": [
    "### 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YMmilm7c1-P",
    "outputId": "1618046e-0bff-4e28-fa9f-e183e3bbcff5"
   },
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# 4-비트 양자화 설정 - QLoRA의 Q 단계\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-비트 정밀도 모델 로드\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 양자화 종류\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # 계산 dtype\n",
    "    bnb_4bit_use_double_quant=True,  # 이중 양자화 적용\n",
    ")\n",
    "\n",
    "# LoRA와 베이스 모델을 합칩니다.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# LLaMA 토크나이저를 로드합니다.\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iidCbaXMs1O4"
   },
   "source": [
    "### 설정"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6IfkvLkylVD",
    "outputId": "3d1e5f3d-3824-4b0b-e06e-949d19069c8d"
   },
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,  # LoRA 스케일링\n",
    "    lora_dropout=0.1,  # LoRA 층의 드롭아웃\n",
    "    r=64,  # 랭크\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # 대상 층\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# 훈련을 위해 모델을 준비합니다.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lk-cEEd8nk27"
   },
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# 훈련 매개변수\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=512\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "Pp3tUXhWm0pE",
    "outputId": "cdf0c782-ff1d-4bd2-b6ea-b5e27b3526c6"
   },
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "# DPOTrainer 객체를 만듭니다.\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# DPO로 모델을 미세 튜닝합니다.\n",
    "dpo_trainer.train()\n",
    "\n",
    "# 어댑터를 저장합니다.\n",
    "dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFE4OKFvyLMe",
    "outputId": "21c65316-3a2d-4628-e8d9-b226f35e4f32"
   },
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# LoRA와 베이스 모델을 합칩니다.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sft_model = model.merge_and_unload()\n",
    "\n",
    "# DPO LoRA와 SFT 모델을 합칩니다.\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    sft_model,\n",
    "    \"TinyLlama-1.1B-dpo-qlora\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "dpo_model = dpo_model.merge_and_unload()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAkwJcHYmxr4",
    "outputId": "79f3a7e0-e8ec-45cf-e5e3-97b73c644448"
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 사전에 정의된 프롬프트 템플릿을 사용합니다.\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# 인스트럭션 튜닝된 모델을 실행합니다.\n",
    "pipe = pipeline(task=\"text-generation\", model=dpo_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
