{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>9ì¥ ë©€í‹°ëª¨ë‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸</h1>\n",
    "<i>ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì—ê²Œ ë¹„ì „ ëŠ¥ë ¥ì„ ì¶”ê°€í•˜ê¸°</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter09.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ <[í•¸ì¦ˆì˜¨ LLM](https://tensorflow.blog/handson-llm/)> ì±… 9ì¥ì˜ ì½”ë“œë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
    "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0mWB0gqO_43"
   },
   "source": [
    "---\n",
    "\n",
    "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6N7d_LipEMFr"
   },
   "source": [
    "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "import os\n",
    "import tqdm\n",
    "from transformers.utils import logging\n",
    "\n",
    "# tqdm ë¹„í™œì„±í™”\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
    "\n",
    "logging.disable_progress_bar()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMF9p8qK58Ou"
   },
   "source": [
    "## OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UHUY1IBzZkgy"
   },
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# AIë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "puppy_path = \"https://bit.ly/4jYqmPu\"\n",
    "image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
    "caption = \"a puppy playing in the snow\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "kHOmvsRFZp4B",
    "outputId": "838df14f-b6f7-4558-cb68-84ac350f2a1b"
   },
   "source": [
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpAwAij0at4o"
   },
   "source": [
    "### ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLaZrGSxavbk",
    "outputId": "8f58ad1a-409b-4900-a244-b0dcdd6589a5"
   },
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ê¸°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë©”ì¸ ëª¨ë¸\n",
    "model = CLIPModel.from_pretrained(model_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSx-X0-ecBv_",
    "outputId": "7730f2b0-a36e-45f6-e5eb-cb50b95eb5bf"
   },
   "source": [
    "# ì…ë ¥ì„ í† í°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
    "inputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVyHP2YwdUqB",
    "outputId": "7fb4cc7c-37ee-46f0-a467-0af125cd39f5"
   },
   "source": [
    "# ì…ë ¥ ì•„ì´ë””ë¥¼ í† í°ìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.\n",
    "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEj4b22qeCm8",
    "outputId": "d37d3449-e216-4774-b968-643b08d8b9c0"
   },
   "source": [
    "# í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_embedding = model.get_text_features(**inputs)\n",
    "text_embedding.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwNQpueperGn",
    "outputId": "67485f88-d819-4091-9bcf-5dec8f27a6ec"
   },
   "source": [
    "# ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "processed_image = clip_processor(\n",
    "    text=None, images=image, return_tensors='pt'\n",
    ")['pixel_values']\n",
    "\n",
    "processed_image.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "4P9I7JsGf7Km",
    "outputId": "6037dd4c-1213-437b-c80f-a1cbdee32d40"
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì‹œê°í™”ë¥¼ ì´í•´ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "img = processed_image.squeeze(0)\n",
    "img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
    "img = np.einsum('ijk->jik', img)\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVFva16chlS3",
    "outputId": "c55bc09a-9415-4347-ad85-32ec0b7d3ae9"
   },
   "source": [
    "# ì´ë¯¸ì§€ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "image_embedding = model.get_image_features(processed_image)\n",
    "image_embedding.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IwyPRa0R964",
    "outputId": "081b11c8-9fe4-47d1-9549-1210c8285ebf"
   },
   "source": [
    "# ì„ë² ë”©ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.\n",
    "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "text_embedding = text_embedding.detach().cpu().numpy()\n",
    "image_embedding = image_embedding.detach().cpu().numpy()\n",
    "score = text_embedding @ image_embedding.T\n",
    "score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rKItoQAkQwU"
   },
   "source": [
    "### ì—¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMsrjH7xkSWH"
   },
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# AIë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "cat_path = \"https://bit.ly/42UJXJu\"\n",
    "car_path = \"https://bit.ly/4cR4rHs\"\n",
    "paths = [puppy_path, cat_path, car_path]\n",
    "images = [Image.open(urlopen(path)).convert(\"RGBA\") for path in paths]\n",
    "captions = [\n",
    "    \"a puppy playing in the snow\",\n",
    "    \"a pixelated image of a cute cat\",\n",
    "    \"A supercar on the road \\nwith the sunset in the background\"\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ëª¨ë“  ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "image_embeddings = []\n",
    "for image in images:\n",
    "  image_processed = clip_processor(images=image, return_tensors='pt')['pixel_values']\n",
    "  image_embedding = model.get_image_features(image_processed).detach().cpu().numpy()[0]\n",
    "  image_embeddings.append(image_embedding)\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "\n",
    "# ëª¨ë“  ìº¡ì…˜ì˜ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_embeddings = []\n",
    "for caption in captions:\n",
    "  inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
    "  text_emb = model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
    "  text_embeddings.append(text_emb)\n",
    "text_embeddings = np.array(text_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F5ABSnFGlGeW"
   },
   "source": [
    "# ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(image_embeddings, text_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cmIyzyTXlIRN",
    "outputId": "126d74f9-d3cf-4231-aad7-06afbba1a7c1"
   },
   "source": [
    "# í”¼ê²¨ ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(sim_matrix, cmap='viridis')\n",
    "\n",
    "# ì •ë‹µ ë ˆì´ë¸”ë¡œ í‹±ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "plt.yticks(range(len(captions)), captions, fontsize=18)\n",
    "plt.xticks([])\n",
    "\n",
    "# ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "for i, image in enumerate(images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "\n",
    "# ìº¡ì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "for x in range(sim_matrix.shape[1]):\n",
    "    for y in range(sim_matrix.shape[0]):\n",
    "        plt.text(x, y, f\"{sim_matrix[y, x]:.2f}\", ha=\"center\", va=\"center\", size=30)\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "# í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
    "plt.xlim([-0.5, len(captions) - 0.5])\n",
    "plt.ylim([len(captions) + 0.5, -2])\n",
    "# plt.title(\"Similarity Matrix\", size=20)\n",
    "plt.savefig(\"sim_matrix.png\", dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5it3Vqi0Bf5"
   },
   "source": [
    "### SBERT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XFEvSss0DQf",
    "outputId": "0863256b-4ab2-45f7-ce51-10d1abd615fb"
   },
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# SBERT í˜¸í™˜ CLIP ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\n",
    "image_embeddings = model.encode(images)\n",
    "\n",
    "# ìº¡ì…˜ì„ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\n",
    "text_embeddings = model.encode(captions)\n",
    "\n",
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "sim_matrix = util.cos_sim(image_embeddings, text_embeddings)\n",
    "print(sim_matrix)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsLAEG2a5-5r"
   },
   "source": [
    "## BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jnNrafWu6BSJ"
   },
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# ì „ì²˜ë¦¬ê¸°ì™€ ë©”ì¸ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "# # Choose specific model because of: https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/39\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ì„ GPUì— ì „ì†¡í•©ë‹ˆë‹¤.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI_UWlR8c_Ey"
   },
   "source": [
    "### ì´ë¯¸ì§€ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "W6YeV_TEQFAA",
    "outputId": "6dbc53f6-7ba6-4ca2-8710-7e845f452cc7"
   },
   "source": [
    "# ìˆ˜í¼ì¹´ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "car_path = \"https://bit.ly/4cR4rHs\"\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flZeiDFmQIIg",
    "outputId": "ce3bd4f6-19db-4c0e-f9ac-836c091385c2"
   },
   "source": [
    "# ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "inputs[\"pixel_values\"].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "0lHJ7LiKUhWf",
    "outputId": "82c0d360-312f-43d3-e7a2-f286c06ae1aa"
   },
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def show_image(image_inputs):\n",
    "    # ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜í•˜ê³ , í¬ê¸°ë¥¼ (1, 3, 224, 224)ì—ì„œ (224, 224, 3)ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "    image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
    "    image_inputs = np.einsum('ijk->kji', image_inputs)\n",
    "    image_inputs = np.einsum('ijk->jik', image_inputs)\n",
    "\n",
    "    # RGB ê°’ì— í•´ë‹¹í•˜ëŠ” 0-255 ë²”ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "    image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
    "    image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
    "\n",
    "    # ë„˜íŒŒì´ ë°°ì—´ì„ Image ê°ì²´ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "    return Image.fromarray(image_inputs)\n",
    "\n",
    "show_image(inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoBrN56dBTF"
   },
   "source": [
    "### í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHJkauW9dFhB",
    "outputId": "00b12d0b-40b1-41e4-beba-16391681e4a1"
   },
   "source": [
    "blip_processor.tokenizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21Zh5Rx8QLAy",
    "outputId": "5c4ed090-5573-4011-ed17-81131d3bfb8f"
   },
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "text = \"Her vocalization was remarkably melodic\"\n",
    "token_ids = blip_processor(text=text, return_tensors=\"pt\")\n",
    "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
    "\n",
    "# ì…ë ¥ IDë¥¼ í† í°ìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.\n",
    "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFnL_Yt3elPX",
    "outputId": "663b92d4-04f3-4ae0-83a4-bf4528272295"
   },
   "source": [
    "# íŠ¹ìˆ˜ í† í°ì„ ë°‘ì¤„ ë¬¸ìë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "tokens = [token.replace(\"Ä \", \"_\") for token in tokens]\n",
    "tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDx5elnHeHnT"
   },
   "source": [
    "### ì‚¬ìš© ì‚¬ë¡€ 1: ì´ë¯¸ì§€ ìº¡ì…”ë‹"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "gFJ9-l8c8u3i",
    "outputId": "ea867e88-3dae-4cb0-c4b9-edf6fe93fe28"
   },
   "source": [
    "# AIê°€ ìƒì„±í•œ ìˆ˜í¼ì¹´ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "\n",
    "# ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "show_image(inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "YHIoTZa6eEaR",
    "outputId": "0fd345b4-73ff-49f6-b11e-efdc4f04e2b2"
   },
   "source": [
    "# ì´ë¯¸ì§€ë¥¼ ì„ë² ë”©ì„ ë§Œë“¤ê³  Q-í¬ë¨¸ì˜ ì¶œë ¥ì„ ë””ì½”ë”(LLM)ì— ì „ë‹¬í•´ í† í° IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# í† í° IDë¥¼ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "JbGnyiYhlfQi",
    "outputId": "0f7f95b0-10ed-47ce-a778-8730f65cd4db"
   },
   "source": [
    "url = \"https://bit.ly/3GJmrra\"\n",
    "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-bIxPEySrJW0",
    "outputId": "5684cc20-efbf-47fe-8c3e-a7812b8f9764"
   },
   "source": [
    "# ë¡œë¥´ìƒ¤í ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "url = \"https://bit.ly/3GJmrra\"\n",
    "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
    "\n",
    "# ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjHfQxWGkVYF"
   },
   "source": [
    "### ì‚¬ìš© ì‚¬ë¡€ 2: ì±„íŒ… ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "269NtauNFIze"
   },
   "source": [
    "# AIë¡œ ìƒì„±í•œ ìˆ˜í¼ì¹´ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "_NsMF_hMCIIj",
    "outputId": "9769b923-8d6e-4176-adb8-7390375feb65"
   },
   "source": [
    "# ì‹œê° ì§ˆë¬¸ ë‹µë³€\n",
    "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
    "\n",
    "# ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë‘ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "NtpMIgUaScBD",
    "outputId": "d4dfd9b9-5f2c-4f9a-82bd-29267fee7a55"
   },
   "source": [
    "# ì±„íŒ… ìŠ¤íƒ€ì¼ì˜ í”„ë¡¬í”„íŠ¸\n",
    "prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"\n",
    "\n",
    "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b511e19f60d04601b9207fd3fc4776a1",
      "053af76c15664b06b65acb74e7d70d14",
      "7bdaed957c3247a29569d7c8e605f525",
      "9e150322299f42b9a528c97dce649b1a",
      "329ad2c718a242efbd5672ed86b9cffb",
      "655766198492461cb54439aaad74adf9",
      "6f2f870b56394a038091e916e4c681d8"
     ]
    },
    "id": "PtvYbcF-H_t6",
    "outputId": "469a7913-9c82-446a-ae51-355c36cbc4c0"
   },
   "source": [
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def text_eventhandler(*args):\n",
    "  question = args[0][\"new\"]\n",
    "  if question:\n",
    "    args[0][\"owner\"].value = \"\"\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    if not memory:\n",
    "      prompt = \" Question: \" + question + \" Answer:\"\n",
    "    else:\n",
    "      template = \"Question: {} Answer: {}.\"\n",
    "      prompt = \" \".join(\n",
    "          [\n",
    "              template.format(memory[i][0], memory[i][1])\n",
    "              for i in range(len(memory))\n",
    "          ]\n",
    "      ) + \" Question: \" + question + \" Answer:\"\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    inputs = blip_processor(image, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    generated_text = blip_processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    generated_text = generated_text[0].strip().split(\"Answer: \")[-1]\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "    memory.append((question, generated_text))\n",
    "\n",
    "    # ì¶œë ¥ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n",
    "    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n",
    "    output.append_display_data(HTML(\"<br>\"))\n",
    "\n",
    "# ìœ„ì ¯ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "in_text = widgets.Text()\n",
    "in_text.continuous_update = False\n",
    "in_text.observe(text_eventhandler, \"value\")\n",
    "output = widgets.Output()\n",
    "memory = []\n",
    "\n",
    "# ì±„íŒ… ìƒìë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        children=[output, in_text],\n",
    "        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
