{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>9장 멀티모달 대규모 언어 모델</h1>\n",
    "<i>대규모 언어 모델에게 비전 능력을 추가하기</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter09.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "이 노트북은 <[핸즈온 LLM](https://tensorflow.blog/handson-llm/)> 책 9장의 코드를 담고 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
    "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0mWB0gqO_43"
   },
   "source": [
    "---\n",
    "\n",
    "💡 **NOTE**: 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6N7d_LipEMFr"
   },
   "source": [
    "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
    "import os\n",
    "import tqdm\n",
    "from transformers.utils import logging\n",
    "\n",
    "# tqdm 비활성화\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
    "\n",
    "logging.disable_progress_bar()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMF9p8qK58Ou"
   },
   "source": [
    "## OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UHUY1IBzZkgy"
   },
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# AI로 생성한 이미지를 로드합니다.\n",
    "puppy_path = \"https://bit.ly/4jYqmPu\"\n",
    "image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
    "caption = \"a puppy playing in the snow\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "kHOmvsRFZp4B",
    "outputId": "838df14f-b6f7-4558-cb68-84ac350f2a1b"
   },
   "source": [
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpAwAij0at4o"
   },
   "source": [
    "### 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLaZrGSxavbk",
    "outputId": "8f58ad1a-409b-4900-a244-b0dcdd6589a5"
   },
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# 텍스트 전처리를 위한 토크나이저를 로드합니다.\n",
    "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# 이미지 전처리를 위한 전처리기를 로드합니다.\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 텍스트 임베딩과 이미지 임베딩을 생성하기 위한 메인 모델\n",
    "model = CLIPModel.from_pretrained(model_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSx-X0-ecBv_",
    "outputId": "7730f2b0-a36e-45f6-e5eb-cb50b95eb5bf"
   },
   "source": [
    "# 입력을 토큰으로 나눕니다.\n",
    "inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
    "inputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVyHP2YwdUqB",
    "outputId": "7fb4cc7c-37ee-46f0-a467-0af125cd39f5"
   },
   "source": [
    "# 입력 아이디를 토큰으로 되돌립니다.\n",
    "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEj4b22qeCm8",
    "outputId": "d37d3449-e216-4774-b968-643b08d8b9c0"
   },
   "source": [
    "# 텍스트 임베딩을 만듭니다.\n",
    "text_embedding = model.get_text_features(**inputs)\n",
    "text_embedding.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwNQpueperGn",
    "outputId": "67485f88-d819-4091-9bcf-5dec8f27a6ec"
   },
   "source": [
    "# 이미지를 전처리합니다.\n",
    "processed_image = clip_processor(\n",
    "    text=None, images=image, return_tensors='pt'\n",
    ")['pixel_values']\n",
    "\n",
    "processed_image.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "4P9I7JsGf7Km",
    "outputId": "6037dd4c-1213-437b-c80f-a1cbdee32d40"
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시각화를 이해 이미지를 준비합니다.\n",
    "img = processed_image.squeeze(0)\n",
    "img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
    "img = np.einsum('ijk->jik', img)\n",
    "\n",
    "# 전처리된 이미지를 출력합니다.\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVFva16chlS3",
    "outputId": "c55bc09a-9415-4347-ad85-32ec0b7d3ae9"
   },
   "source": [
    "# 이미지 임베딩을 만듭니다.\n",
    "image_embedding = model.get_image_features(processed_image)\n",
    "image_embedding.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IwyPRa0R964",
    "outputId": "081b11c8-9fe4-47d1-9549-1210c8285ebf"
   },
   "source": [
    "# 임베딩을 정규화합니다.\n",
    "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# 유사도를 계산합니다.\n",
    "text_embedding = text_embedding.detach().cpu().numpy()\n",
    "image_embedding = image_embedding.detach().cpu().numpy()\n",
    "score = text_embedding @ image_embedding.T\n",
    "score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rKItoQAkQwU"
   },
   "source": [
    "### 여러 이미지로 유사도 점수 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMsrjH7xkSWH"
   },
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# AI로 생성한 이미지를 로드합니다.\n",
    "cat_path = \"https://bit.ly/42UJXJu\"\n",
    "car_path = \"https://bit.ly/4cR4rHs\"\n",
    "paths = [puppy_path, cat_path, car_path]\n",
    "images = [Image.open(urlopen(path)).convert(\"RGBA\") for path in paths]\n",
    "captions = [\n",
    "    \"a puppy playing in the snow\",\n",
    "    \"a pixelated image of a cute cat\",\n",
    "    \"A supercar on the road \\nwith the sunset in the background\"\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 모든 이미지의 임베딩을 만듭니다.\n",
    "image_embeddings = []\n",
    "for image in images:\n",
    "  image_processed = clip_processor(images=image, return_tensors='pt')['pixel_values']\n",
    "  image_embedding = model.get_image_features(image_processed).detach().cpu().numpy()[0]\n",
    "  image_embeddings.append(image_embedding)\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "\n",
    "# 모든 캡션의 임베딩을 만듭니다.\n",
    "text_embeddings = []\n",
    "for caption in captions:\n",
    "  inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
    "  text_emb = model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
    "  text_embeddings.append(text_emb)\n",
    "text_embeddings = np.array(text_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F5ABSnFGlGeW"
   },
   "source": [
    "# 이미지와 캡션의 코사인 유사도를 계산합니다.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(image_embeddings, text_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cmIyzyTXlIRN",
    "outputId": "126d74f9-d3cf-4231-aad7-06afbba1a7c1"
   },
   "source": [
    "# 피겨 객체를 만듭니다.\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(sim_matrix, cmap='viridis')\n",
    "\n",
    "# 정답 레이블로 틱을 설정합니다.\n",
    "plt.yticks(range(len(captions)), captions, fontsize=18)\n",
    "plt.xticks([])\n",
    "\n",
    "# 이미지를 출력합니다.\n",
    "for i, image in enumerate(images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "\n",
    "# 캡션을 추가합니다.\n",
    "for x in range(sim_matrix.shape[1]):\n",
    "    for y in range(sim_matrix.shape[0]):\n",
    "        plt.text(x, y, f\"{sim_matrix[y, x]:.2f}\", ha=\"center\", va=\"center\", size=30)\n",
    "\n",
    "# 불필요한 요소를 제거합니다.\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "# 크기를 조정합니다.\n",
    "plt.xlim([-0.5, len(captions) - 0.5])\n",
    "plt.ylim([len(captions) + 0.5, -2])\n",
    "# plt.title(\"Similarity Matrix\", size=20)\n",
    "plt.savefig(\"sim_matrix.png\", dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5it3Vqi0Bf5"
   },
   "source": [
    "### SBERT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XFEvSss0DQf",
    "outputId": "0863256b-4ab2-45f7-ce51-10d1abd615fb"
   },
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# SBERT 호환 CLIP 모델을 로드합니다.\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# 이미지를 인코딩합니다.\n",
    "image_embeddings = model.encode(images)\n",
    "\n",
    "# 캡션을 인코딩합니다.\n",
    "text_embeddings = model.encode(captions)\n",
    "\n",
    "# 코사인 유사도를 계산합니다.\n",
    "sim_matrix = util.cos_sim(image_embeddings, text_embeddings)\n",
    "print(sim_matrix)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsLAEG2a5-5r"
   },
   "source": [
    "## BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jnNrafWu6BSJ"
   },
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# 전처리기와 메인 모델을 로드합니다.\n",
    "# # Choose specific model because of: https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/39\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 추론 속도를 높이기 위해 모델을 GPU에 전송합니다.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI_UWlR8c_Ey"
   },
   "source": [
    "### 이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "W6YeV_TEQFAA",
    "outputId": "6dbc53f6-7ba6-4ca2-8710-7e845f452cc7"
   },
   "source": [
    "# 수퍼카 이미지를 로드합니다.\n",
    "car_path = \"https://bit.ly/4cR4rHs\"\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flZeiDFmQIIg",
    "outputId": "ce3bd4f6-19db-4c0e-f9ac-836c091385c2"
   },
   "source": [
    "# 이미지를 전처리합니다.\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "inputs[\"pixel_values\"].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "0lHJ7LiKUhWf",
    "outputId": "82c0d360-312f-43d3-e7a2-f286c06ae1aa"
   },
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def show_image(image_inputs):\n",
    "    # 넘파이 배열로 변환하고, 크기를 (1, 3, 224, 224)에서 (224, 224, 3)로 바꿉니다.\n",
    "    image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
    "    image_inputs = np.einsum('ijk->kji', image_inputs)\n",
    "    image_inputs = np.einsum('ijk->jik', image_inputs)\n",
    "\n",
    "    # RGB 값에 해당하는 0-255 범위로 변환합니다.\n",
    "    scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "    image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
    "    image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
    "\n",
    "    # 넘파이 배열을 Image 객체로 바꿉니다.\n",
    "    return Image.fromarray(image_inputs)\n",
    "\n",
    "show_image(inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoBrN56dBTF"
   },
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHJkauW9dFhB",
    "outputId": "00b12d0b-40b1-41e4-beba-16391681e4a1"
   },
   "source": [
    "blip_processor.tokenizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21Zh5Rx8QLAy",
    "outputId": "5c4ed090-5573-4011-ed17-81131d3bfb8f"
   },
   "source": [
    "# 텍스트를 전처리합니다.\n",
    "text = \"Her vocalization was remarkably melodic\"\n",
    "token_ids = blip_processor(text=text, return_tensors=\"pt\")\n",
    "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
    "\n",
    "# 입력 ID를 토큰으로 되돌립니다.\n",
    "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFnL_Yt3elPX",
    "outputId": "663b92d4-04f3-4ae0-83a4-bf4528272295"
   },
   "source": [
    "# 특수 토큰을 밑줄 문자로 바꿉니다.\n",
    "tokens = [token.replace(\"Ġ\", \"_\") for token in tokens]\n",
    "tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDx5elnHeHnT"
   },
   "source": [
    "### 사용 사례 1: 이미지 캡셔닝"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "gFJ9-l8c8u3i",
    "outputId": "ea867e88-3dae-4cb0-c4b9-edf6fe93fe28"
   },
   "source": [
    "# AI가 생성한 수퍼카 이미지를 로드합니다.\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "\n",
    "# 이미지를 전처리하여 입력을 준비합니다.\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "show_image(inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "YHIoTZa6eEaR",
    "outputId": "0fd345b4-73ff-49f6-b11e-efdc4f04e2b2"
   },
   "source": [
    "# 이미지를 임베딩을 만들고 Q-포머의 출력을 디코더(LLM)에 전달해 토큰 ID를 생성합니다.\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# 토큰 ID를 바탕으로 텍스트를 생성합니다.\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "JbGnyiYhlfQi",
    "outputId": "0f7f95b0-10ed-47ce-a778-8730f65cd4db"
   },
   "source": [
    "url = \"https://bit.ly/3GJmrra\"\n",
    "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-bIxPEySrJW0",
    "outputId": "5684cc20-efbf-47fe-8c3e-a7812b8f9764"
   },
   "source": [
    "# 로르샤흐 이미지를 로드합니다.\n",
    "url = \"https://bit.ly/3GJmrra\"\n",
    "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
    "\n",
    "# 캡션을 생성합니다.\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjHfQxWGkVYF"
   },
   "source": [
    "### 사용 사례 2: 채팅 기반 멀티모달 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "269NtauNFIze"
   },
   "source": [
    "# AI로 생성한 수퍼카 이미지를 로드합니다.\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "_NsMF_hMCIIj",
    "outputId": "9769b923-8d6e-4176-adb8-7390375feb65"
   },
   "source": [
    "# 시각 질문 답변\n",
    "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
    "\n",
    "# 이미지와 프롬프트를 모두 전처리합니다.\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# 텍스트를 생성합니다.\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "NtpMIgUaScBD",
    "outputId": "d4dfd9b9-5f2c-4f9a-82bd-29267fee7a55"
   },
   "source": [
    "# 채팅 스타일의 프롬프트\n",
    "prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"\n",
    "\n",
    "# 출력을 생성합니다.\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b511e19f60d04601b9207fd3fc4776a1",
      "053af76c15664b06b65acb74e7d70d14",
      "7bdaed957c3247a29569d7c8e605f525",
      "9e150322299f42b9a528c97dce649b1a",
      "329ad2c718a242efbd5672ed86b9cffb",
      "655766198492461cb54439aaad74adf9",
      "6f2f870b56394a038091e916e4c681d8"
     ]
    },
    "id": "PtvYbcF-H_t6",
    "outputId": "469a7913-9c82-446a-ae51-355c36cbc4c0"
   },
   "source": [
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def text_eventhandler(*args):\n",
    "  question = args[0][\"new\"]\n",
    "  if question:\n",
    "    args[0][\"owner\"].value = \"\"\n",
    "\n",
    "    # 프롬프트를 만듭니다.\n",
    "    if not memory:\n",
    "      prompt = \" Question: \" + question + \" Answer:\"\n",
    "    else:\n",
    "      template = \"Question: {} Answer: {}.\"\n",
    "      prompt = \" \".join(\n",
    "          [\n",
    "              template.format(memory[i][0], memory[i][1])\n",
    "              for i in range(len(memory))\n",
    "          ]\n",
    "      ) + \" Question: \" + question + \" Answer:\"\n",
    "\n",
    "    # 텍스트를 생성합니다.\n",
    "    inputs = blip_processor(image, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    generated_text = blip_processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    generated_text = generated_text[0].strip().split(\"Answer: \")[-1]\n",
    "\n",
    "    # 메모리를 업데이트합니다.\n",
    "    memory.append((question, generated_text))\n",
    "\n",
    "    # 출력에 할당합니다.\n",
    "    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n",
    "    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n",
    "    output.append_display_data(HTML(\"<br>\"))\n",
    "\n",
    "# 위젯을 준비합니다.\n",
    "in_text = widgets.Text()\n",
    "in_text.continuous_update = False\n",
    "in_text.observe(text_eventhandler, \"value\")\n",
    "output = widgets.Output()\n",
    "memory = []\n",
    "\n",
    "# 채팅 상자를 출력합니다.\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        children=[output, in_text],\n",
    "        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
