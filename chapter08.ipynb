{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A2SZPmbD4Pk"
   },
   "source": [
    "<h1>8장 시맨틱 검색과 RAG</h1>\n",
    "<i>LLM의 핵심 요소인 검색을 살펴 봅니다.</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter08.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "이 노트북은 <[핸즈온 LLM](https://tensorflow.blog/handson-llm/)> 책 8장의 코드를 담고 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
    "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1HLGL7ihy1E"
   },
   "source": [
    "### [선택사항] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>에서 패키지 선택하기\n",
    "\n",
    "\n",
    "이 노트북을 구글 코랩에서 실행한다면 다음 코드 셀을 실행하여 이 노트북에서 필요한 패키지를  설치하세요.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
    "import os\n",
    "import tqdm\n",
    "from transformers.utils import logging\n",
    "\n",
    "# tqdm 비활성화\n",
    "# tqdm.tqdm = lambda *args, **kwargs: iter([])\n",
    "# tqdm.auto.tqdm = lambda *args, **kwargs: iter([])\n",
    "# tqdm.notebook.tqdm = lambda *args, **kwargs: iter([])\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
    "\n",
    "logging.disable_progress_bar()"
   ],
   "metadata": {
    "id": "il-RhVV7-kbo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zSImFYZAhy1E"
   },
   "source": [
    "%%capture\n",
    "!pip install cohere faiss-cpu rank_bm25 langchain-community\n",
    "\n",
    "# 사용하는 파이썬과 CUDA 버전에 맞는 llama-cpp-python 패키지를 설치하세요.\n",
    "# 현재 코랩의 파이썬 버전은 3.11이며 CUDA 버전은 12.4입니다.\n",
    "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye0HbBr3EV0P"
   },
   "source": [
    "## 밀집 검색 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svgdo3y3F741"
   },
   "source": [
    "### 1. 텍스트 문서 분할하기\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uOFFg7YWFoaf"
   },
   "source": [
    "import cohere\n",
    "\n",
    "# 코히어의 API 키를 입력하세요.\n",
    "api_key = 'hrnLgkfhcwNuYseUyoUP1D2DO6lCSxSkb2jNyC14'\n",
    "\n",
    "# 코히어 클라이언트를 만듭니다.\n",
    "co = cohere.Client(api_key)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_Dcq1j_xFxIr"
   },
   "source": [
    "text = \"\"\"\n",
    "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
    "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
    "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
    "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
    "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
    "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
    "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
    "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
    "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
    "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
    "\n",
    "# 문장을 나누어 리스트로 만듭니다.\n",
    "texts = text.split('.')\n",
    "\n",
    "# 공백과 줄바꿈 문자를 삭제합니다.\n",
    "texts = [t.strip(' \\n') for t in texts]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krDDOpcZF5qo"
   },
   "source": [
    "### 2. 문장을 임베딩하기\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xooetZg0Fz4K",
    "outputId": "2967278a-dcd9-4d86-e50e-f81a6070cc2b"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# 임베딩을 만듭니다.\n",
    "response = co.embed(\n",
    "  texts=texts,\n",
    "  input_type=\"search_document\",\n",
    ").embeddings\n",
    "\n",
    "embeds = np.array(response)\n",
    "print(embeds.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLVdFg1PF4GG"
   },
   "source": [
    "### 3. 검색 인덱스 구축하기\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JyqzN2-JF24N"
   },
   "source": [
    "import faiss\n",
    "\n",
    "dim = embeds.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.float32(embeds))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6qRFo8dGGrJ"
   },
   "source": [
    "### 4. 인덱스 검색하기\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o83pxM5sGHxp"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def search(query, number_of_results=3):\n",
    "\n",
    "  # 1. 쿼리의 임베딩을 만듭니다.\n",
    "  query_embed = co.embed(texts=[query],\n",
    "                input_type=\"search_query\",).embeddings[0]\n",
    "\n",
    "  # 2. 최근접 이웃을 추출합니다.\n",
    "  distances, similar_item_ids = index.search(np.float32([query_embed]),\n",
    "                                             number_of_results)\n",
    "\n",
    "  # 3. 데이터프레임을 사용해 출력을 준비합니다.\n",
    "  texts_np = np.array(texts) # 인덱싱을 쉽게 하기 위해 텍스트 리스트를 넘파이 배열로 변환합니다.\n",
    "  results = pd.DataFrame(data={'텍스트': texts_np[similar_item_ids[0]],\n",
    "                               '거리': distances[0]})\n",
    "\n",
    "  # 4. 결과를 출력하고 반환합니다.\n",
    "  print(f\"쿼리:'{query}'\\n최근접 이웃:\")\n",
    "  return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "Rq_2knm_GLgR",
    "outputId": "47274550-f9c6-43f9-b8be-ca7c351b3e7e"
   },
   "source": [
    "query = \"how precise was the science\"\n",
    "results = search(query)\n",
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EkkDh12ZGRhY"
   },
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHl8HnvgGXHG",
    "outputId": "00f70f10-5070-470f-8f84-a1962d8f546f"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(texts):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZlyGXye4GRj0"
   },
   "source": [
    "def keyword_search(query, top_k=3, num_candidates=15):\n",
    "    print(\"입력 질문:\", query)\n",
    "\n",
    "    ##### BM25 검색 (어휘 검색) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(f\"탑-3 어휘 검색 (BM25) 결과\")\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jV-V_mhRGRmS",
    "outputId": "3f2ad2df-eba2-4a21-aecc-6c0eb303819d"
   },
   "source": [
    "keyword_search(query = \"how precise was the science\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehyhfd7NG5kw"
   },
   "source": [
    "### 밀집 검색의 단점\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "NxYwEfYRGpNe",
    "outputId": "64173232-8f28-4765-887a-563bdd828e14"
   },
   "source": [
    "query = \"What is the mass of the moon?\"\n",
    "results = search(query)\n",
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_RalLmuG0jw"
   },
   "source": [
    "## 리랭킹 예제\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HulOxkW_Focv",
    "outputId": "c63fc2f5-1b7f-416f-c185-21870b5e7b1c"
   },
   "source": [
    "query = \"how precise was the science\"\n",
    "results = co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
    "results.results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUrmMW8LFofP",
    "outputId": "f31ca01d-c31b-4abd-8363-3fba9e04c6b9"
   },
   "source": [
    "for idx, result in enumerate(results.results):\n",
    "    print(idx, result.relevance_score , result.document.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rqYJaq2CFohv"
   },
   "source": [
    "def keyword_and_reranking_search(query, top_k=3, num_candidates=10):\n",
    "    print(\"입력 질문:\", query)\n",
    "\n",
    "    ##### BM25 검색 (어휘 검색) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(f\"탑-3 어휘 검색 (BM25) 결과\")\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # 리랭킹 추가\n",
    "    docs = [texts[hit['corpus_id']] for hit in bm25_hits]\n",
    "\n",
    "    print(f\"\\n리랭킹으로 얻은 탑-3 결과 ({len(bm25_hits)}개의 BM25 결과를 재조정함)\")\n",
    "    results = co.rerank(query=query, documents=docs, top_n=top_k, return_documents=True)\n",
    "    for hit in results.results:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.document.text.replace(\"\\n\", \" \")))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FITOXqkHONy",
    "outputId": "6bd210f4-0355-46c9-9753-b448fbd798e7"
   },
   "source": [
    "keyword_and_reranking_search(query = \"how precise was the science\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugdnTs_VHV25"
   },
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iqKQ7F0HZh-"
   },
   "source": [
    "### 예: LLM API를 사용한 근거 기반 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeHX0D8DHaim",
    "outputId": "98c0abc2-0ed0-4b01-99d4-2b10eed58fdd"
   },
   "source": [
    "query = \"income generated\"\n",
    "\n",
    "# 1- 검색\n",
    "# 여기서는 임베딩 검색을 사용하지만 하이브리드 방식이 이상적입니다.\n",
    "results = search(query)\n",
    "\n",
    "# 2- 근거 기반 생성\n",
    "docs_dict = [{'text': text} for text in results['텍스트']]\n",
    "response = co.chat(\n",
    "    message = query,\n",
    "    documents=docs_dict\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9YmHEOHHpUW",
    "outputId": "fe1e26e7-5ade-426e-883f-f3136ec387f9"
   },
   "source": [
    "response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLwaGXM2Hg7b",
    "outputId": "3bf9e8b2-1d92-4ae9-b43b-edb42618c8c7"
   },
   "source": [
    "response.citations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_25ztzEHuWX"
   },
   "source": [
    "### 예: 로컬 모델을 사용한 RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNZ5gUoWIYhp"
   },
   "source": [
    "#### 생성 모델 로드하기\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4LNwOWTHvOv",
    "outputId": "e4a8edca-27ac-444b-b94e-98f02b1a69e3"
   },
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2Qgnc5OHvRQ",
    "outputId": "af19e761-5b21-46ac-f3b9-ab8f910f7a80"
   },
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "# 여러분의 컴퓨터에 다운로드한 모델의 경로를 입력하세요!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=4096,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7ahQtlvIZjS"
   },
   "source": [
    "#### 임베딩 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODkBMgsIIddp",
    "outputId": "a0c92b2d-dc54-4c4b-ebaa-1b657907f92e"
   },
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 텍스트를 수치 표현으로 변환하기 위한 임베딩 모델\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-small-en-v1.5'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgPua3jsIgmW"
   },
   "source": [
    "#### 벡터 데이터베이스 구축"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NV57LOf8IjM-"
   },
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 로컬 벡터 데이터베이스를 만듭니다.\n",
    "db = FAISS.from_texts(texts, embedding_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P06UYeIVIk1e"
   },
   "source": [
    "#### RAG 프롬프트\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F_3nTc69InwO"
   },
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# 프롬프트 템플릿을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Relevant information:\n",
    "{context}\n",
    "\n",
    "Provide a concise answer the following question using the relevant information provided above:\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# RAG 파이프라인\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt\n",
    "    },\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2p2pJPfIp16",
    "outputId": "4a163a5c-773b-403e-9d08-db6c16130463"
   },
   "source": [
    "rag.invoke('Income generated')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
