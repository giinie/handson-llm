{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>7장 고급 텍스트 생성 기술과 도구</h1>\n",
    "<i>프롬프트 엔지니어링을 넘어서</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter07.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "이 노트북은 <[핸즈온 LLM](https://tensorflow.blog/handson-llm/)> 책 7장의 코드를 담고 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
    "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtUx27GOCAYd"
   },
   "source": [
    "### [선택사항] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>에서 패키지 선택하기\n",
    "\n",
    "\n",
    "이 노트북을 구글 코랩에서 실행한다면 다음 코드 셀을 실행하여 이 노트북에서 필요한 패키지를  설치하세요.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
    "import os\n",
    "import tqdm\n",
    "from transformers.utils import logging\n",
    "\n",
    "# tqdm 비활성화\n",
    "tqdm.tqdm = lambda *args, **kwargs: iter([])\n",
    "tqdm.auto.tqdm = lambda *args, **kwargs: iter([])\n",
    "tqdm.notebook.tqdm = lambda *args, **kwargs: iter([])\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
    "\n",
    "logging.disable_progress_bar()"
   ],
   "metadata": {
    "id": "ILBN7AMQ0SfM"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Txh47zAxCAYd"
   },
   "source": [
    "%%capture\n",
    "!pip install langchain_community langchain_openai duckduckgo-search\n",
    "\n",
    "# 사용하는 파이썬과 CUDA 버전에 맞는 llama-cpp-python 패키지를 설치하세요.\n",
    "# 현재 코랩의 파이썬 버전은 3.11이며 CUDA 버전은 12.4입니다.\n",
    "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rerbJgwAigbK"
   },
   "source": [
    "# LLM 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EYKJi4bCAYf",
    "outputId": "2fb3d05c-bb34-4214-c909-2c903dab1469"
   },
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQcht_ZFijW7",
    "outputId": "79746405-eff5-477c-8b97-edbf21a61005"
   },
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "# 여러분의 컴퓨터에 다운로드한 모델의 경로를 입력하세요!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=4096,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3SNhQF9WthzV",
    "outputId": "fb7b2b19-ac91-4308-bc3b-586769ea928c"
   },
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwx2AIuGfCoP"
   },
   "source": [
    "## 체인"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kF--Q5me_-X1"
   },
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# \"input_prompt\" 변수를 가진 프롬프트 템플릿을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ogWsGeg6hElt"
   },
   "source": [
    "basic_chain = prompt | llm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KINQxKAINXgG",
    "outputId": "db64b3e1-d0cc-42c3-8c9e-5683f1140e32"
   },
   "source": [
    "# 체인을 사용합니다.\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMBMRxB8gFW"
   },
   "source": [
    "### 여러 템플릿을 가진 체인"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrUKuHt_OLpe",
    "outputId": "b65545de-9bce-4970-c622-0befcb4ef8c1"
   },
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "# 이야기 제목을 위한 체인을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igFIyg73OtaL",
    "outputId": "2a7113da-fb93-4b70-8703-347b09d51230"
   },
   "source": [
    "title.invoke({\"summary\": \"a girl that lost her mother\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zTtFEmANOhyE"
   },
   "source": [
    "# 요약과 제목을 사용하여 캐릭터 설명을 생성하는 체인을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}.\n",
    "Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xjf-avW8NAqZ"
   },
   "source": [
    "# 요약, 제목, 캐릭터 설명을 사용해 이야기를 생성하는 체인을 만듭니다.\n",
    "template = \"\"\"<|user|>\n",
    "Create a story about {summary} with the title {title}.\n",
    "The main charachter is: {character}.\n",
    "Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "epNudKyyPClO"
   },
   "source": [
    "# 세 개의 요소를 연결하여 최종 체인을 만듭니다.\n",
    "llm_chain = title | character | story"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b44ZR0vXRaAo",
    "outputId": "b9751173-1dcf-431f-b43a-7659013a6aae"
   },
   "source": [
    "llm_chain.invoke(\"a girl that lost her mother\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UQ-DZ71P-D-"
   },
   "source": [
    "# 메모리"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-15Eoey5EJUO",
    "outputId": "3eda9bf7-31d1-4215-a7a8-2cce34721fa1"
   },
   "source": [
    "# LLM에게 이름을 알려 줍니다.\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "N42wQRl-Lykt",
    "outputId": "c399add1-e309-417e-c225-0f51aa754719"
   },
   "source": [
    "# LLM에게 이름을 묻습니다.\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfqATEZjMgET"
   },
   "source": [
    "### 대화 버퍼"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zoo0PA1fUs70"
   },
   "source": [
    "# 대화 기록을 담을 수 있도록 프롬프트를 업데이트합니다.\n",
    "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
    "\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgGMS1S9saLi",
    "outputId": "facd6321-09eb-405f-d53f-4bb8ff6f1662"
   },
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 사용할 메모리를 정의합니다.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# LLM, 프롬프트, 메모리를 연결합니다.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mltR_GtkiqDZ",
    "outputId": "f75bb690-1275-4525-c26d-079ada5b6831"
   },
   "source": [
    "# 간단한 질문을 하여 대화 기록을 만듭니다.\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-je1rmy3dx4",
    "outputId": "231fafa4-c233-4ead-f50f-0fe4f99e5aca"
   },
   "source": [
    "# LLM이 이름을 기억할까요?\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3ELCg6Rpsk"
   },
   "source": [
    "### 윈도 대화 버퍼"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0DRT7kjRtiC",
    "outputId": "ea324a12-46a8-4585-f472-557b7e58ba9b"
   },
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 메모리에 마지막 두 개의 대화만 유지합니다.\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "# LLM, 프롬프트, 메모리를 연결합니다.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBY69vvcR1Qq",
    "outputId": "f1090de7-d296-49c9-f19b-64242c1b5fc3"
   },
   "source": [
    "# 두 개의 질문을 던져 메모리에 대화 기록을 저장합니다.\n",
    "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvSLfKWpR5h5",
    "outputId": "6845c17e-384b-495e-ebb9-0a6699ea74ed"
   },
   "source": [
    "# 이름을 기억하는고 있는지 확인합니다.\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW7qEyctcqeJ",
    "outputId": "a5b398bc-c5ff-4001-8ec2-1eff34eac85d"
   },
   "source": [
    "# 이름을 기억하는고 있는지 확인합니다.\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSb5OnANMhu2"
   },
   "source": [
    "### 대화 요약"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lWHZlJUbwpqE"
   },
   "source": [
    "# 요약 프롬프트 템플릿을 만듭니다.\n",
    "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg1HAgxZMkbO",
    "outputId": "61d85942-dee1-4511-f6c2-0c6f5fd2d410"
   },
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# 사용할 메모리를 정의합니다.\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# LLM, 프롬프트, 메모리를 연결합니다.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2klIk9CpVSH0",
    "outputId": "37b45670-5f41-445f-ca9f-ec042b6f2494"
   },
   "source": [
    "# 이름에 대해 질문하는 대화를 생성합니다.\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VdOH_I-V-Fy",
    "outputId": "de109966-6cfe-4807-f935-81d3351b9376"
   },
   "source": [
    "# 지금까지 내용이 요약되어 있는지 확인합니다.\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1_LlvrVX9HL",
    "outputId": "6b5d365d-5859-4587-fe81-762a8ed66416"
   },
   "source": [
    "# 지금까지 요약 내용을 확인합니다.\n",
    "memory.load_memory_variables({})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG5sJa1qvS4N"
   },
   "source": [
    "# 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rcBt8bZM56dM"
   },
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 랭체인으로 오픈AI의 LLM을 로드합니다.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lmRZu8DO2p6k"
   },
   "source": [
    "# ReAct 템플릿을 만듭니다.\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NV-ssNa-4zOK"
   },
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# 에이전트에 전달할 도구를 만듭니다.\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# 도구를 준비합니다.\n",
    "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
    "tools.append(search_tool)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6tAr1962vS4T"
   },
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# ReAct 에이전트를 만듭니다.\n",
    "agent = create_react_agent(openai_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSU6ECdYBOOm",
    "outputId": "44c7607a-4f45-4b45-810a-547a6632a712"
   },
   "source": [
    "# 맥북 프로의 가격은 얼마인가요?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
