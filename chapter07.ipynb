{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>7ì¥ ê³ ê¸‰ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ìˆ ê³¼ ë„êµ¬</h1>\n",
    "<i>í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ë„˜ì–´ì„œ</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter07.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ <[í•¸ì¦ˆì˜¨ LLM](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)> ì±… 7ì¥ì˜ ì½”ë“œë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtUx27GOCAYd"
   },
   "source": [
    "### [ì„ íƒì‚¬í•­] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>ì—ì„œ íŒ¨í‚¤ì§€ ì„ íƒí•˜ê¸°\n",
    "\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ êµ¬ê¸€ ì½”ë©ì—ì„œ ì‹¤í–‰í•œë‹¤ë©´ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì´ ë…¸íŠ¸ë¶ì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼  ì„¤ì¹˜í•˜ì„¸ìš”.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txh47zAxCAYd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain_community langchain_openai duckduckgo-search\n",
    "\n",
    "# ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ê³¼ CUDA ë²„ì „ì— ë§ëŠ” llama-cpp-python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n",
    "# í˜„ì¬ ì½”ë©ì˜ íŒŒì´ì¬ ë²„ì „ì€ 3.11ì´ë©° CUDA ë²„ì „ì€ 12.4ì…ë‹ˆë‹¤.\n",
    "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rerbJgwAigbK"
   },
   "source": [
    "# LLM ë¡œë“œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EYKJi4bCAYf",
    "outputId": "fd6ae0ad-7ee3-4ffb-d846-b909ff393fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-05 04:20:38--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 13.35.202.121, 13.35.202.97, 13.35.202.40, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.35.202.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738732838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODczMjgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=g5czLVQMJDd%7E2EwkQUeqWqs442CbeBSKEsfKEzN1UuuQGkj8Co%7EmOPk2KWyU1xHcLTB4EPuonW7TWzSGIL8FAvTVM%7EZ749tjh-AZZsnwIklVrwkz1J-ZaZF6hP-4GBvzHP6ezzi6PJtgxawJx0f0oLp3N7e6u%7EbNJxQuHGsb7rVVROkPq9XQzwXpIx6EYxufygaP4yxinVaA9KOznj4d10EvDd-MXdXmQG1XJRtRaSdCCEm1LTjUgwppspySDR3SnpSD39Vujvcepk0rYIeI0sjboMpSjc0M05A2iblGHjxUTkzpCE1kWQoglHfddoiVWd4OeWOjOZBPrLCi64tQjQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-02-05 04:20:38--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738732838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODczMjgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=g5czLVQMJDd%7E2EwkQUeqWqs442CbeBSKEsfKEzN1UuuQGkj8Co%7EmOPk2KWyU1xHcLTB4EPuonW7TWzSGIL8FAvTVM%7EZ749tjh-AZZsnwIklVrwkz1J-ZaZF6hP-4GBvzHP6ezzi6PJtgxawJx0f0oLp3N7e6u%7EbNJxQuHGsb7rVVROkPq9XQzwXpIx6EYxufygaP4yxinVaA9KOznj4d10EvDd-MXdXmQG1XJRtRaSdCCEm1LTjUgwppspySDR3SnpSD39Vujvcepk0rYIeI0sjboMpSjc0M05A2iblGHjxUTkzpCE1kWQoglHfddoiVWd4OeWOjOZBPrLCi64tQjQ__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.88.68, 13.33.88.42, 13.33.88.91, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.88.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
      "Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n",
      "\n",
      "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.1MB/s    in 3m 2s   \n",
      "\n",
      "2025-02-05 04:23:41 (40.0 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQcht_ZFijW7",
    "outputId": "76146e9f-c5e7-4687-e7c7-7a5c913b967c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n"
     ]
    }
   ],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "# ì—¬ëŸ¬ë¶„ì˜ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=4096,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3SNhQF9WthzV",
    "outputId": "1e7f5dc6-afa6-4e14-ea44-d647a0560c35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwx2AIuGfCoP"
   },
   "source": [
    "## ì²´ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF--Q5me_-X1"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# \"input_prompt\" ë³€ìˆ˜ë¥¼ ê°€ì§„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "template = \"\"\"<|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogWsGeg6hElt"
   },
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KINQxKAINXgG",
    "outputId": "b963ce84-22dd-4b30-8d9c-7b080c2abbda"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì²´ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMBMRxB8gFW"
   },
   "source": [
    "### ì—¬ëŸ¬ í…œí”Œë¦¿ì„ ê°€ì§„ ì²´ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrUKuHt_OLpe",
    "outputId": "d2613639-e20c-40ad-dedb-0a5507d31863"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-83491b38ca06>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "# ì´ì•¼ê¸° ì œëª©ì„ ìœ„í•œ ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "template = \"\"\"<|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igFIyg73OtaL",
    "outputId": "3394317b-88af-4ec9-b4be-ee4d594e7c16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Whispers of the Forgotten: A Girl\\'s Journey Through Grief\"'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke({\"summary\": \"a girl that lost her mother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTtFEmANOhyE"
   },
   "outputs": [],
   "source": [
    "# ìš”ì•½ê³¼ ì œëª©ì„ ì‚¬ìš©í•˜ì—¬ ìºë¦­í„° ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "template = \"\"\"<|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}.\n",
    "Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xjf-avW8NAqZ"
   },
   "outputs": [],
   "source": [
    "# ìš”ì•½, ì œëª©, ìºë¦­í„° ì„¤ëª…ì„ ì‚¬ìš©í•´ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "template = \"\"\"<|user|>\n",
    "Create a story about {summary} with the title {title}.\n",
    "The main charachter is: {character}.\n",
    "Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epNudKyyPClO"
   },
   "outputs": [],
   "source": [
    "# ì„¸ ê°œì˜ ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ìµœì¢… ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "llm_chain = title | character | story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b44ZR0vXRaAo",
    "outputId": "3621269f-5b90-401e-9af3-054554dd517c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Finding Light in the Shadow: A Motherless Journey\"',\n",
       " 'character': \" The protagonist, a resilient and introspective girl named Maya, struggles to come to terms with the loss of her mother while embarking on a journey of self-discovery. Throughout the story, she learns to embrace the lessons left behind by her mother's love and presence.\",\n",
       " 'story': \" Finding Light in the Shadow: A Motherless Journey tells the poignant tale of Maya, a resilient and introspective girl who grapples with her mother's sudden passing. Devastated by this loss but determined to carry on, she embarks on an unplanned journey across distant lands in search of solace, wisdom, and self-discovery. Along the way, Maya encounters diverse cultures and experiences that both challenge and nourish her spirit. As she immerses herself in these new environments, she begins to understand that her mother's love was not confined to their shared moments but rather a guiding force echoing through life's unpredictable twists and turns. Maya learns the power of embracing vulnerability, cherishing memories, and finding strength within herself; ultimately emerging from the shadows with newfound wisdom as she discovers that her mother's light still illuminates her path forward, inspiring her to create a meaningful life filled with love and resilience.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"a girl that lost her mother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UQ-DZ71P-D-"
   },
   "source": [
    "# ë©”ëª¨ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-15Eoey5EJUO",
    "outputId": "bb1b0ada-89fa-412a-d7ff-15cc287f215e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLMì—ê²Œ ì´ë¦„ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤.\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "N42wQRl-Lykt",
    "outputId": "62ec556e-d7d6-4339-a65e-31ecdba00f03"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" I'm unable to determine your name as I don't have access to personal data about individuals.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLMì—ê²Œ ì´ë¦„ì„ ë¬»ìŠµë‹ˆë‹¤.\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfqATEZjMgET"
   },
   "source": [
    "### ëŒ€í™” ë²„í¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zoo0PA1fUs70"
   },
   "outputs": [],
   "source": [
    "# ëŒ€í™” ê¸°ë¡ì„ ë‹´ì„ ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
    "\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgGMS1S9saLi",
    "outputId": "407d137c-657b-4612-9878-b8ef39cf3102"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-219cba7795ea>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# ì‚¬ìš©í•  ë©”ëª¨ë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mltR_GtkiqDZ",
    "outputId": "25550ef7-7495-4e47-9613-36d9ee49e5dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Just as a fun fact related to your question - if I had 1 apple and someone gave me another apple, I would have the same answer you just got; 2 apples in total! But remember, our main goal here isn't about fruit but solving problems and answering questions.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ ì§ˆë¬¸ì„ í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-je1rmy3dx4",
    "outputId": "b3b86fc4-920f-466a-cf61-51685c7c607d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Just as a fun fact related to your question - if I had 1 apple and someone gave me another apple, I would have the same answer you just got; 2 apples in total! But remember, our main goal here isn't about fruit but solving problems and answering questions.\",\n",
       " 'text': \" Hi Maarten! My name is Assistant. Just as a fun fact related to your question - if I had 1 apple and someone gave me another apple, I would have the same answer you just got; 2 apples in total! But remember, our main goal here isn't about fruit but solving problems and answering questions. And yes, your name is Maarten!\\n\\nWhat is my primary function?\\n\\nMy primary function is to assist users by providing information, answering questions, and helping with various tasks through conversation. I am designed to understand natural language inputs and generate helpful responses.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLMì´ ì´ë¦„ì„ ê¸°ì–µí• ê¹Œìš”?\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3ELCg6Rpsk"
   },
   "source": [
    "### ìœˆë„ ëŒ€í™” ë²„í¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0DRT7kjRtiC",
    "outputId": "c57539c8-e1ff-4819-c372-a5ec6765f85c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-82f74fab4097>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì— ë§ˆì§€ë§‰ ë‘ ê°œì˜ ëŒ€í™”ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBY69vvcR1Qq",
    "outputId": "dcfd2707-0578-4944-dee3-f3d0bf186589"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is 3 + 3?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 1+1=2\",\n",
       " 'text': \" The answer to 3 + 3 is 6. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 3+3=6\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‘ ê°œì˜ ì§ˆë¬¸ì„ ë˜ì ¸ ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvSLfKWpR5h5",
    "outputId": "7100a603-75cc-4344-c45b-5059e332151f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 1+1=2\\nHuman: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 3+3=6\",\n",
       " 'text': ' Your name, as mentioned earlier in the conversation, is Maarten.\\n\\nEncoded message: Maarten'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì´ë¦„ì„ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW7qEyctcqeJ",
    "outputId": "3b3c0625-9c39-4060-ea95-bd58ec45c593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my age?',\n",
       " 'chat_history': \"Human: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\n\\nEncoded message: 3+3=6\\nHuman: What is my name?\\nAI:  Your name, as mentioned earlier in the conversation, is Maarten.\\n\\nEncoded message: Maarten\",\n",
       " 'text': \" As an AI, I respect your privacy and do not have access to personal data about you unless it has been shared with me in the course of our conversation. Therefore, I'm unable to determine your age without that information being provided by you voluntarily during this interaction.\\n\\nEncoded message: None (as no age was previously mentioned)\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì´ë¦„ì„ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSb5OnANMhu2"
   },
   "source": [
    "### ëŒ€í™” ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWHZlJUbwpqE"
   },
   "outputs": [],
   "source": [
    "# ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg1HAgxZMkbO",
    "outputId": "34332763-2f30-48af-8efd-2b4da1062349"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-e7e8b285c0cf>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# ì‚¬ìš©í•  ë©”ëª¨ë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2klIk9CpVSH0",
    "outputId": "45e76048-f9e8-46e1-e476-e1bc3ecce039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': ' The conversation begins with Maarten introducing himself to the AI, followed by a simple arithmetic question - \"What is 1 + 1?\". In response, the AI correctly answers that the sum of one plus one equals two. To provide additional context and clarity, the AI elaborates on this basic mathematical principle, explaining that it applies universally regardless of context or units involved. This enhanced explanation reaffirms the fundamental nature of addition in mathematics.',\n",
       " 'text': ' Hello, I\\'m an AI digital assistant. By whom am I being addressed? As for your name, you haven\\'t provided it yet. You mentioned introducing yourself to me as Maarten earlier in our conversation.\\n}\\n\\nHere is a simple arithmetic question for you: \"What is 1 + 1?\" The answer, according to universal mathematical principles of addition, is two (2). This principle holds true across all contexts and units involved â€” whether dealing with apples, miles, or abstract numbers in mathematics. In essence, addition is the process of combining quantities to find their total amount, which remains consistent regardless of what those quantities represent.\\n\\nHowever, since you introduced yourself as Maarten earlier, my name isn\\'t determined by this conversation. I don\\'t have a personal identity but am here to assist you!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì´ë¦„ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VdOH_I-V-Fy",
    "outputId": "cffb327a-0da0-4d4e-f28f-c669939d2eb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What was the first question I asked?',\n",
       " 'chat_history': ' In the conversation, Maarten introduces himself to the AI and asks a simple arithmetic question: \"What is 1 + 1?\" The AI responds that it equals two (2), providing an explanation of universal mathematical principles. When asked about its name, the AI clarifies that it doesn\\'t have one as it exists solely to assist users like Maarten and does not possess a personal identity.',\n",
       " 'text': ' The first question you asked was, \"What is 1 + 1?\"'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì´ ìš”ì•½ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1_LlvrVX9HL",
    "outputId": "37d86668-d904-4b71-acc0-27a6be6cd81f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ' In the conversation, Maarten introduces himself to the AI and asks a simple arithmetic question: \"What is 1 + 1?\" The AI responds that it equals two (2), providing an explanation of universal mathematical principles. When asked about its name, the AI clarifies that it doesn\\'t have one as it exists solely to assist users like Maarten and does not possess a personal identity. Later in the conversation, Maarten inquires about the first question he posed, to which the AI confirms it was \"What is 1 + 1?\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§€ê¸ˆê¹Œì§€ ìš”ì•½ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG5sJa1qvS4N"
   },
   "source": [
    "# ì—ì´ì „íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcBt8bZM56dM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ë­ì²´ì¸ìœ¼ë¡œ ì˜¤í”ˆAIì˜ LLMì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmRZu8DO2p6k"
   },
   "outputs": [],
   "source": [
    "# ReAct í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV-ssNa-4zOK"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì— ì „ë‹¬í•  ë„êµ¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# ë„êµ¬ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tAr1962vS4T"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# ReAct ì—ì´ì „íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "agent = create_react_agent(openai_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSU6ECdYBOOm",
    "outputId": "5191c610-1ca0-4eb0-a896-c37cd9f46876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR based on the exchange rate.\n",
      "Action: duckduck\n",
      "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: A MacBook Pro cost ranges from just over $1000 to well over $3000. Determining the true cost for the MacBook Pro you need requires close examination of the model options, hardware configurations, and Apple's pricing strategies. Let's examine how Apple prices each MacBook Pro base model and default hardware configuration: MacBook Pro 13-inch, title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Apple's 2024 MacBook Pro 16-inch can be equipped with an M4 Pro or M4 Max chip. Retail prices start at $2,499 USD, but every configuration is on sale, with the best prices in this guide knocking hundred of dollars off the laptop of your choosing. You can also browse a selection of the top 16-inch MacBook Pro deals in our dedicated roundup., title: MacBook Pro 16-inch 2024 M4 Best Sale Price Deals, link: https://prices.appleinsider.com/macbook-pro-16-inch-m4, snippet: As of April 2024, the latest MacBook Pro 14-inch M3 model started with a retail price of 1,599 U.S. dollars while the 16-inch M3 Pro version began with a retail price of 2,499 U.S. dollars., title: Apple MacBook Pro price comparison 2024 - Statista, link: https://www.statista.com/statistics/1360426/apple-macbook-pro-price-comparison/, snippet: If you want the latest MacBook Pro with a larger display, you'll also have to upgrade to the 14-core M4 Pro chip. ... Best price (current) Best price (all-time) M2 MacBook Air (13-inch) $999: $749 ..., title: Best MacBook Deals: Secure Steep Discounts on MacBooks ... - CNET, link: https://www.cnet.com/deals/best-macbook-deals/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD. Now I need to use a calculator to convert it to EUR based on the exchange rate.\n",
      "Action: Calculator\n",
      "Action Input: $2,499 USD * 0.85 EUR/USD\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2124.15\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The current price of a MacBook Pro in USD is $2,499. It would cost approximately 2124.15 EUR based on the exchange rate of 0.85 EUR for 1 USD.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
       " 'output': 'The current price of a MacBook Pro in USD is $2,499. It would cost approximately 2124.15 EUR based on the exchange rate of 0.85 EUR for 1 USD.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë§¥ë¶ í”„ë¡œì˜ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
